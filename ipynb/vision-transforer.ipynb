{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar e importar dependencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseado no argtigo: https://www.akshaymakes.com/blogs/vision-transformer\n",
    "\n",
    "import torch, requests, os\n",
    "from torch import nn\n",
    "from torchinfo import summary\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "from torchvision.transforms import Resize, Compose, ToTensor\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar os arquivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL for the zip file\n",
    "url = \"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\"\n",
    "\n",
    "# Send a GET request to download the file\n",
    "response = requests.get(url)\n",
    "\n",
    "# Define the path to the data directory\n",
    "data_path = Path(\"data\")\n",
    "\n",
    "# Define the path to the image directory\n",
    "image_path = data_path / \"pizza_steak_sushi\"\n",
    "\n",
    "# Check if the image directory already exists\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory, creating one...\")\n",
    "    image_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write the downloaded content to a zip file\n",
    "with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "# Extract the contents of the zip file to the image directory\n",
    "with ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zipref:\n",
    "    zipref.extractall(image_path)\n",
    "\n",
    "# Remove the downloaded zip file\n",
    "os.remove(data_path / 'pizza_steak_sushi.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define as transformações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the train_transform using Compose\n",
    "train_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "# Define the test_transform using Compose\n",
    "test_transform = Compose([\n",
    "    Resize((224, 224)),\n",
    "    ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria dataset e o dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data directory\n",
    "data_dir = Path(\"data/pizza_steak_sushi\")\n",
    "\n",
    "# Create the training dataset using ImageFolder\n",
    "training_dataset = ImageFolder(root=data_dir / \"train\", transform=train_transform)\n",
    "\n",
    "# Create the test dataset using ImageFolder\n",
    "test_dataset = ImageFolder(root=data_dir / \"test\", transform=test_transform)\n",
    "\n",
    "# Create the training dataloader using DataLoader\n",
    "training_dataloader = DataLoader(\n",
    "    dataset=training_dataset,\n",
    "    shuffle=True,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# Create the test dataloader using DataLoader\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    shuffle=False,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = 5\n",
    "num_cols = num_rows\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(num_rows, num_cols, figsize=(10, 10))\n",
    "\n",
    "# Iterate over the subplots and display random images from the training dataset\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        # Choose a random index from the training dataset\n",
    "        image_index = random.randrange(len(training_dataset))\n",
    "\n",
    "        # Display the image in the subplot\n",
    "        axs[i, j].imshow(training_dataset[image_index][0].permute((1, 2, 0)))\n",
    "\n",
    "        # Set the title of the subplot as the corresponding class name\n",
    "        axs[i, j].set_title(training_dataset.classes[training_dataset[image_index][1]], color=\"white\")\n",
    "\n",
    "        # Disable the axis for better visualization\n",
    "        axs[i, j].axis(False)\n",
    "\n",
    "# Set the super title of the figure\n",
    "fig.suptitle(f\"Random {num_rows * num_cols} images from the training dataset\", fontsize=16, color=\"white\")\n",
    "\n",
    "# Set the background color of the figure as black\n",
    "fig.set_facecolor(color='black')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria o Layer Patch Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATCH_SIZE = 16\n",
    "IMAGE_WIDTH = 224\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH\n",
    "IMAGE_CHANNELS = 3\n",
    "EMBEDDING_DIMS = IMAGE_CHANNELS * PATCH_SIZE**2\n",
    "NUM_OF_PATCHES = int((IMAGE_WIDTH * IMAGE_HEIGHT) / PATCH_SIZE**2)\n",
    "\n",
    "#the image width and image height should be divisible by patch size. This is a check to see that.\n",
    "\n",
    "assert IMAGE_WIDTH % PATCH_SIZE == 0 and IMAGE_HEIGHT % PATCH_SIZE == 0 , print(\"Image Width is not divisible by patch size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_layer = nn.Conv2d(in_channels = IMAGE_CHANNELS, out_channels = EMBEDDING_DIMS, kernel_size = PATCH_SIZE, stride = PATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_images, random_labels = next(iter(training_dataloader))\n",
    "random_image = random_images[0]\n",
    "\n",
    "# Create a new figure\n",
    "fig = plt.figure(1)\n",
    "\n",
    "# Display the random image\n",
    "plt.imshow(random_image.permute((1, 2, 0)))\n",
    "\n",
    "# Disable the axis for better visualization\n",
    "plt.axis(False)\n",
    "\n",
    "# Set the title of the image\n",
    "plt.title(training_dataset.classes[random_labels[0]], color=\"white\")\n",
    "\n",
    "# Set the background color of the figure as black\n",
    "fig.set_facecolor(color=\"black\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the image through the convolution layer\n",
    "image_through_conv = conv_layer(random_image.unsqueeze(0))\n",
    "print(f'Shape of embeddings through the conv layer -> {list(image_through_conv.shape)} <- [batch_size, num_of_patch_rows,num_patch_cols embedding_dims]')\n",
    "\n",
    "# Permute the dimensions of image_through_conv to match the expected shape\n",
    "image_through_conv = image_through_conv.permute((0, 2, 3, 1))\n",
    "\n",
    "# Create a flatten layer using nn.Flatten\n",
    "flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
    "\n",
    "# Pass the image_through_conv through the flatten layer\n",
    "image_through_conv_and_flatten = flatten_layer(image_through_conv)\n",
    "\n",
    "# Print the shape of the embedded image\n",
    "print(f'Shape of embeddings through the flatten layer -> {list(image_through_conv_and_flatten.shape)} <- [batch_size, num_of_patches, embedding_dims]')\n",
    "\n",
    "# Assign the embedded image to a variable\n",
    "embedded_image = image_through_conv_and_flatten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepara a classe de tokenizar os patchs e adicioar a posição dos embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_token_embeddings = nn.Parameter(torch.rand((1, 1,EMBEDDING_DIMS), requires_grad  = True))\n",
    "print(f'Shape of class_token_embeddings --> {list(class_token_embeddings.shape)} <-- [batch_size, 1, emdedding_dims]')\n",
    "\n",
    "embedded_image_with_class_token_embeddings = torch.cat((class_token_embeddings, embedded_image), dim = 1)\n",
    "print(f'\\nShape of image embeddings with class_token_embeddings --> {list(embedded_image_with_class_token_embeddings.shape)} <-- [batch_size, num_of_patches+1, embeddiing_dims]')\n",
    "\n",
    "position_embeddings = nn.Parameter(torch.rand((1, NUM_OF_PATCHES+1, EMBEDDING_DIMS ), requires_grad = True ))\n",
    "print(f'\\nShape of position_embeddings --> {list(position_embeddings.shape)} <-- [batch_size, num_patches+1, embeddings_dims]')\n",
    "\n",
    "final_embeddings = embedded_image_with_class_token_embeddings + position_embeddings\n",
    "print(f'\\nShape of final_embeddings --> {list(final_embeddings.shape)} <-- [batch_size, num_patches+1, embeddings_dims]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colocar o Layer junto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbeddingLayer(nn.Module):\n",
    "    def __init__(self, in_channels, patch_size, embedding_dim,):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.in_channels = in_channels\n",
    "        self.conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=embedding_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        self.flatten_layer = nn.Flatten(start_dim=1, end_dim=2)\n",
    "        self.class_token_embeddings = nn.Parameter(torch.rand((BATCH_SIZE, 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "        self.position_embeddings = nn.Parameter(torch.rand((1, NUM_OF_PATCHES + 1, EMBEDDING_DIMS), requires_grad=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = torch.cat((self.class_token_embeddings, self.flatten_layer(self.conv_layer(x).permute((0, 2, 3, 1)))), dim=1) + self.position_embeddings\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_embedding_layer = PatchEmbeddingLayer(in_channels=IMAGE_CHANNELS, patch_size=PATCH_SIZE, embedding_dim=IMAGE_CHANNELS * PATCH_SIZE ** 2)\n",
    "\n",
    "patch_embeddings = patch_embedding_layer(random_images)\n",
    "patch_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=patch_embedding_layer,\n",
    "    input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, input_channels, img_width, img_height)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cria o multi head auto atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self,\n",
    "    embedding_dims = 768, # Hidden Size D in the ViT Paper Table 1\n",
    "    num_heads = 12,  # Heads in the ViT Paper Table 1\n",
    "    attn_dropout = 0.0 # Default to Zero as there is no dropout for the the MSA Block as per the ViT Paper\n",
    "            ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.embedding_dims = embedding_dims\n",
    "    self.num_head = num_heads\n",
    "    self.attn_dropout = attn_dropout\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
    "\n",
    "    self.multiheadattention =  nn.MultiheadAttention(\n",
    "        num_heads = num_heads,\n",
    "        embed_dim = embedding_dims,\n",
    "        dropout = attn_dropout,\n",
    "        batch_first = True,\n",
    "                                                    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.layernorm(x)\n",
    "    output,_ = self.multiheadattention(\n",
    "        query=x, \n",
    "        key=x, \n",
    "        value=x,\n",
    "        need_weights=False\n",
    "    )\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o bloco MSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_self_attention_block = MultiHeadSelfAttentionBlock(\n",
    "    embedding_dims = EMBEDDING_DIMS,\n",
    "    num_heads = 12\n",
    "                                                             )\n",
    "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
    "print(f'Shape of the output from MSA Block => {list(multihead_self_attention_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=multihead_self_attention_block,\n",
    "    input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o bloco do perceptron (MLP = Machine Learning Perceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MachineLearningPerceptronBlock(nn.Module):\n",
    "  def __init__(self, embedding_dims, mlp_size, mlp_dropout):\n",
    "    super().__init__()\n",
    "    self.embedding_dims = embedding_dims\n",
    "    self.mlp_size = mlp_size\n",
    "    self.dropout = mlp_dropout\n",
    "\n",
    "    self.layernorm = nn.LayerNorm(normalized_shape = embedding_dims)\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(in_features = embedding_dims, out_features = mlp_size),\n",
    "        nn.GELU(),\n",
    "        nn.Dropout(p = mlp_dropout),\n",
    "        nn.Linear(in_features = mlp_size, out_features = embedding_dims),\n",
    "        nn.Dropout(p = mlp_dropout)\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.mlp(self.layernorm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp_block = MachineLearningPerceptronBlock(\n",
    "    embedding_dims = EMBEDDING_DIMS,\n",
    "    mlp_size = 3072,\n",
    "    mlp_dropout = 0.1\n",
    "    )\n",
    "\n",
    "summary(\n",
    "    model=mlp_block,\n",
    "    input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anexando o Bloco do transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    embedding_dims = 768,\n",
    "    mlp_dropout=0.1,\n",
    "    attn_dropout=0.0,\n",
    "    mlp_size = 3072,\n",
    "    num_heads = 12,\n",
    "                ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.msa_block = MultiHeadSelfAttentionBlock(\n",
    "        embedding_dims = embedding_dims,\n",
    "        num_heads = num_heads,\n",
    "        attn_dropout = attn_dropout\n",
    "                                                )\n",
    "\n",
    "    self.mlp_block = MachineLearningPerceptronBlock(\n",
    "        embedding_dims = embedding_dims,\n",
    "        mlp_size = mlp_size,\n",
    "        mlp_dropout = mlp_dropout,\n",
    "                                                    )\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = self.msa_block(x) + x\n",
    "    x = self.mlp_block(x) + x\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testando o Bloco do transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_block = TransformerBlock(\n",
    "    embedding_dims = EMBEDDING_DIMS,\n",
    "    mlp_dropout = 0.1,\n",
    "    attn_dropout=0.0,\n",
    "    mlp_size = 3072,\n",
    "    num_heads = 12\n",
    "                                     )\n",
    "\n",
    "print(f'Shape of the input Patch Embeddings => {list(patch_embeddings.shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n",
    "print(f'Shape of the output from Transformer Block => {list(transformer_block(patch_embeddings).shape)} <= [batch_size, num_patches+1, embedding_dims ]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=transformer_block,\n",
    "    input_size=(1, 197, 768), # (batch_size, num_patches, embedding_dimension)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o modelo ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "  def __init__(\n",
    "    self, \n",
    "    img_size = 224,\n",
    "    in_channels = 3,\n",
    "    patch_size = 16,\n",
    "    embedding_dims = 768,\n",
    "    num_transformer_layers = 12, # from table 1 above\n",
    "    mlp_dropout = 0.1,\n",
    "    attn_dropout = 0.0,\n",
    "    mlp_size = 3072,\n",
    "    num_heads = 12,\n",
    "    num_classes = 1000\n",
    "               ):\n",
    "    super().__init__()\n",
    "\n",
    "    self.patch_embedding_layer = PatchEmbeddingLayer(\n",
    "        in_channels = in_channels,\n",
    "        patch_size=patch_size,\n",
    "        embedding_dim = embedding_dims\n",
    "                                                     )\n",
    "\n",
    "    self.transformer_encoder = nn.Sequential(*[\n",
    "        TransformerBlock(\n",
    "            embedding_dims = embedding_dims,\n",
    "            mlp_dropout = mlp_dropout,\n",
    "            attn_dropout = attn_dropout,\n",
    "            mlp_size = mlp_size,\n",
    "            num_heads = num_heads\n",
    "            ) \n",
    "        for _ in range(num_transformer_layers)\n",
    "        ])\n",
    "\n",
    "    self.classifier = nn.Sequential(nn.LayerNorm(\n",
    "            normalized_shape = embedding_dims\n",
    "        ),\n",
    "        nn.Linear(\n",
    "            in_features = embedding_dims,\n",
    "            out_features = num_classes)\n",
    "        )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.classifier(self.transformer_encoder(self.patch_embedding_layer(x))[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary(\n",
    "    model=vit,\n",
    "    input_size=(BATCH_SIZE, 3, 224, 224), # (batch_size, num_patches, embedding_dimension)\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
